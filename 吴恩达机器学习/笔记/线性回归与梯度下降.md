
**什么是线性回归**<br>
线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，<br>
运用十分广泛。其表达形式为y = w'x+e，e为误差服从均值为0的正态分布，即给定一堆数据，找出一条直线<br>
来和这堆数据拟合。机器学习中实现线性回归的方法是梯度下降<br><br><br>

**梯度下降**<br>
梯度下降公式为:<br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta_i=\Theta_i-\alpha&space;\frac{\partial&space;}{\partial&space;x}&space;J(\Theta_1,\Theta&space;_2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta_i=\Theta_i-\alpha&space;\frac{\partial&space;}{\partial&space;x}&space;J(\Theta_1,\Theta&space;_2)" title="\Theta_i=\Theta_i-\alpha \frac{\partial }{\partial x} J(\Theta_1,\Theta _2)" /></a>
 <br>
 其中α是步长,α太小则梯度下降的速度太慢，α太大则可能跳过局部带价函数最低点，J(θ1，θ2)即为代价函数<br>
 
 **代价函数**<br>
 <a href="https://www.codecogs.com/eqnedit.php?latex=min=\frac{1}{2m}\sum_{1}^{m}(h\Theta&space;(x_i)-y_i)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?min=\frac{1}{2m}\sum_{1}^{m}(h\Theta&space;(x_i)-y_i)^2" title="min=\frac{1}{2m}\sum_{1}^{m}(h\Theta (x_i)-y_i)^2" /></a>
<br>为何代价函数是除以2m而非m，因为无论除以哪个，最后所获得的的θ都是相同的，所以用2m做分母方便求偏微分<br><br><br>

**多元线性回归**<br>
<a href="https://www.codecogs.com/eqnedit.php?latex=h(\Theta)=\Theta_1X_1&plus;\Theta_2X_2&plus;\cdot&space;\cdot&space;\cdot&space;&plus;\Theta_nX_n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h(\Theta)=\Theta_1X_1&plus;\Theta_2X_2&plus;\cdot&space;\cdot&space;\cdot&space;&plus;\Theta_nX_n" title="h(\Theta)=\Theta_1X_1+\Theta_2X_2+\cdot \cdot \cdot +\Theta_nX_n" /></a><br>
多元线性回归也可以用梯度下降解法，同时还可以用正规方程，首先我们来了解梯度下降的缩放问题:<br>
梯度下降缩放主要是因为特征值之间的尺度相差太大，例如特征值x1~(0,1),x2~(0,1000)会使得带价函数等高线变得<br>
瘦高或者肥矮，会严重影响递归下降的效率，例如z=x1θ1+x2θ2，若x1为1，x2为1000，则求θ2的偏微分时，由于采用<br>
相同学习率α，θ2的步长会迈的比θ1大得多，是下降不能按照较优的路线进行，那么缩放过后不会影响θ的取值吗？<br>
我认为应该同除以2m一样，关乎停止梯度下降找到局部最优的是偏导数为0，缩放只影响偏导数的常系数，对偏导数<br>
是否为0不影响，特征值缩放的方法一般为：x=(x-μ)/Sn  其中Sn为均方差，μ为均值<br>
***但我一直没有想通，梯度下降时，θ的初始值如何选择呢？可以随你假设一个初始值，我实现的代码中用的0***
<br><br>

正规方程：正规方程即利用矩阵方法，将所有特征值与结果构成矩阵:<br>
![image](https://github.com/yeeeex/black-hole/blob/master/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%80%E4%BA%9B%E5%85%AC%E5%BC%8F/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E7%9F%A9%E9%98%B5.gif)<br>
如公式，我们将构建矩阵等式，要求得θ即等式左边左乘矩阵X，***我目前没想明白课程里为何写成了如下格式：***
<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;=(X^TX)^{-1}X^Ty" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;=(X^TX)^{-1}X^Ty" title="\Theta =(X^TX)^{-1}X^Ty" /></a><br>
这个矩阵不是可以直接化简吗？***应该是为了解决不可逆的问题？*** 特征集构成的矩阵可能不为方阵<br>
多元线性回归的代价函数为：<br>
<img src="https://latex.codecogs.com/gif.latex?J(\Theta&space;_1,\Theta&space;_2,\cdot&space;\cdot&space;\cdot,\Theta&space;_n&space;)=\frac{1}{2m}\sum_{i=1}^{m}(h_\Theta&space;(x^{(i)})-y^{(i)})^2" title="J(\Theta _1,\Theta _2,\cdot \cdot \cdot,\Theta _n )=\frac{1}{2m}\sum_{i=1}^{m}(h_\Theta (x^{(i)})-y^{(i)})^2" /><br>
其中上标i是指第i组特征值，hθ(x)是指每个θ和每组特征值中的每个特征值之积之和的结果<br>

***一般来说，特征值的组数大于1000就应该考虑梯度下降的方法，小于等于可以考虑正规方程***

**正规方程推导过程，以特殊值数量=2为例**<br>
代价函数J(θ)可以写成:<br>
<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{2m}(X\Theta-Y)^{T}(X\Theta&space;-Y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{2m}(X\Theta-Y)^{T}(X\Theta&space;-Y)" title="\frac{1}{2m}(X\Theta-Y)^{T}(X\Theta -Y)" /></a> <br>

<a href="https://www.codecogs.com/eqnedit.php?latex==\frac{1}{2m}(\Theta&space;^TX^TX\Theta&space;-\Theta&space;^TX^TY-Y^TX\Theta&space;&plus;Y^TY)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?=\frac{1}{2m}(\Theta&space;^TX^TX\Theta&space;-\Theta&space;^TX^TY-Y^TX\Theta&space;&plus;Y^TY)" title="=\frac{1}{2m}(\Theta ^TX^TX\Theta -\Theta ^TX^TY-Y^TX\Theta +Y^TY)" /></a> <br>


<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;^TX^TY=[\Theta&space;_1,\Theta&space;_2]&space;\begin{bmatrix}&space;x_{11}y_1&plus;x_{21}y_2&plus;\cdot&space;\cdot&space;\cdot&space;&plus;x_{m1}y_m&space;\\&space;x_{12}y_1&plus;x_{22}y_2&plus;\cdot&space;\cdot&space;\cdot&space;&plus;x_{m2}ym&space;\end{bmatrix}\rightarrow&space;\frac{\partial\Theta&space;^TX^TY&space;}{\partial&space;\Theta&space;}=X^TY" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;^TX^TY=[\Theta&space;_1,\Theta&space;_2]&space;\begin{bmatrix}&space;x_{11}y_1&plus;x_{21}y_2&plus;\cdot&space;\cdot&space;\cdot&space;&plus;x_{m1}y_m&space;\\&space;x_{12}y_1&plus;x_{22}y_2&plus;\cdot&space;\cdot&space;\cdot&space;&plus;x_{m2}ym&space;\end{bmatrix}\rightarrow&space;\frac{\partial\Theta&space;^TX^TY&space;}{\partial&space;\Theta&space;}=X^TY" title="\Theta ^TX^TY=[\Theta _1,\Theta _2] \begin{bmatrix} x_{11}y_1+x_{21}y_2+\cdot \cdot \cdot +x_{m1}y_m \\ x_{12}y_1+x_{22}y_2+\cdot \cdot \cdot +x_{m2}ym \end{bmatrix}\rightarrow \frac{\partial\Theta ^TX^TY }{\partial \Theta }=X^TY" /></a>  <br>


<a href="https://www.codecogs.com/eqnedit.php?latex=Y^TX\Theta&space;=\begin{bmatrix}&space;y_1x_{11}&plus;y_2x_{21}&plus;\cdot&space;\cdot&space;\cdot&space;&plus;y_mx_{m1}&space;&y_1x_{12}&plus;y2_x_{22}&plus;\cdot&space;\cdot&space;\cdot&space;y_mx_{m2}&space;\end{bmatrix}\begin{bmatrix}&space;\Theta_1&space;\\&space;\Theta_2&space;\end{bmatrix}\rightarrow&space;\frac{\partial&space;Y^TX\Theta&space;}{\partial&space;\Theta&space;}=X^TY" target="_blank"><img src="https://latex.codecogs.com/gif.latex?Y^TX\Theta&space;=\begin{bmatrix}&space;y_1x_{11}&plus;y_2x_{21}&plus;\cdot&space;\cdot&space;\cdot&space;&plus;y_mx_{m1}&space;&y_1x_{12}&plus;y2_x_{22}&plus;\cdot&space;\cdot&space;\cdot&space;y_mx_{m2}&space;\end{bmatrix}\begin{bmatrix}&space;\Theta_1&space;\\&space;\Theta_2&space;\end{bmatrix}\rightarrow&space;\frac{\partial&space;Y^TX\Theta&space;}{\partial&space;\Theta&space;}=X^TY" title="Y^TX\Theta =\begin{bmatrix} y_1x_{11}+y_2x_{21}+\cdot \cdot \cdot +y_mx_{m1} &y_1x_{12}+y2_x_{22}+\cdot \cdot \cdot y_mx_{m2} \end{bmatrix}\begin{bmatrix} \Theta_1 \\ \Theta_2 \end{bmatrix}\rightarrow \frac{\partial Y^TX\Theta }{\partial \Theta }=X^TY" /></a>
<br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;^TX^TX\Theta&space;=[\Theta_1,\Theta&space;_2]\begin{bmatrix}&space;x_{11}^2&plus;x_{21}^2&plus;\cdot&space;\cdot&space;\cdot&space;&plus;x_{m1}^2&space;&&space;x_{11}x_{12}&plus;x_{21}x_{22}&plus;\cdot&space;\cdot&space;\cdot&space;x_{m1}x_{m2}&space;\\&space;x_{12}x_{11}&plus;x_{22}x_{21}&plus;\cdot&space;\cdot&space;\cdot&space;x_{m2}x_{m1}&space;&&space;x_{12}^2&plus;x_{22}^2&plus;\cdot&space;\cdot&space;\cdot&space;x_{m2}^2&space;\end{bmatrix}&space;\begin{bmatrix}&space;\Theta&space;_1&space;\\&space;\Theta&space;_2&space;\end{bmatrix}\rightarrow&space;\frac{\partial&space;\Theta^TX^TX\Theta&space;}{\partial&space;\Theta&space;}=&space;2\begin{bmatrix}&space;2t_1&space;&t_2&plus;t_3&space;\\&space;t_3&plus;t_2&2t_4&space;\end{bmatrix}\begin{bmatrix}&space;\Theta_1&space;\\&space;\Theta_2&space;\end{bmatrix}&space;\because&space;t2=t3\therefore&space;=\begin{bmatrix}&space;2t_1&space;&2t_2&space;\\&space;2t_3&2t_4&space;\end{bmatrix}=2X^TX\Theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;^TX^TX\Theta&space;=[\Theta_1,\Theta&space;_2]\begin{bmatrix}&space;x_{11}^2&plus;x_{21}^2&plus;\cdot&space;\cdot&space;\cdot&space;&plus;x_{m1}^2&space;&&space;x_{11}x_{12}&plus;x_{21}x_{22}&plus;\cdot&space;\cdot&space;\cdot&space;x_{m1}x_{m2}&space;\\&space;x_{12}x_{11}&plus;x_{22}x_{21}&plus;\cdot&space;\cdot&space;\cdot&space;x_{m2}x_{m1}&space;&&space;x_{12}^2&plus;x_{22}^2&plus;\cdot&space;\cdot&space;\cdot&space;x_{m2}^2&space;\end{bmatrix}&space;\begin{bmatrix}&space;\Theta&space;_1&space;\\&space;\Theta&space;_2&space;\end{bmatrix}\rightarrow&space;\frac{\partial&space;\Theta^TX^TX\Theta&space;}{\partial&space;\Theta&space;}=&space;2\begin{bmatrix}&space;2t_1&space;&t_2&plus;t_3&space;\\&space;t_3&plus;t_2&2t_4&space;\end{bmatrix}\begin{bmatrix}&space;\Theta_1&space;\\&space;\Theta_2&space;\end{bmatrix}&space;\because&space;t2=t3\therefore&space;=\begin{bmatrix}&space;2t_1&space;&2t_2&space;\\&space;2t_3&2t_4&space;\end{bmatrix}=2X^TX\Theta" title="\Theta ^TX^TX\Theta =[\Theta_1,\Theta _2]\begin{bmatrix} x_{11}^2+x_{21}^2+\cdot \cdot \cdot +x_{m1}^2 & x_{11}x_{12}+x_{21}x_{22}+\cdot \cdot \cdot x_{m1}x_{m2} \\ x_{12}x_{11}+x_{22}x_{21}+\cdot \cdot \cdot x_{m2}x_{m1} & x_{12}^2+x_{22}^2+\cdot \cdot \cdot x_{m2}^2 \end{bmatrix} \begin{bmatrix} \Theta _1 \\ \Theta _2 \end{bmatrix}\rightarrow \frac{\partial \Theta^TX^TX\Theta }{\partial \Theta }= 2\begin{bmatrix} 2t_1 &t_2+t_3 \\ t_3+t_2&2t_4 \end{bmatrix}\begin{bmatrix} \Theta_1 \\ \Theta_2 \end{bmatrix} \because t2=t3\therefore =\begin{bmatrix} 2t_1 &2t_2 \\ 2t_3&2t_4 \end{bmatrix}=2X^TX\Theta" /></a>
<br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\therefore&space;\frac{\partial&space;J(\Theta)}{\partial&space;\Theta&space;}=\frac{1}{2m}(2X^TX\Theta&space;-X^TY-X^TY)&space;=0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\therefore&space;\frac{\partial&space;J(\Theta)}{\partial&space;\Theta&space;}=\frac{1}{2m}(2X^TX\Theta&space;-X^TY-X^TY)&space;=0" title="\therefore \frac{\partial J(\Theta)}{\partial \Theta }=\frac{1}{2m}(2X^TX\Theta -X^TY-X^TY) =0" /></a>
<br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\therefore&space;2X^TX\Theta&space;=2X^TY\rightarrow&space;\Theta&space;=(X^TX)^{-1}X^TY" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\therefore&space;2X^TX\Theta&space;=2X^TY\rightarrow&space;\Theta&space;=(X^TX)^{-1}X^TY" title="\therefore 2X^TX\Theta =2X^TY\rightarrow \Theta =(X^TX)^{-1}X^TY" /></a>
<br>
***我最初认为正规方程是通过 XΘ=Y--->Θ=X^(-1)Y的来的，但是事实是(R(X)=2) != (R(X,Y)=3) 此矩阵是无解只是在非奇异矩阵时***<br>
***((X^T)X)^(-1)(X^T)Y=(X^-1)Y 这个等式给我造成的错觉，实际上这个等式几乎是无法成立的，所以其实Θ在正规方程中不是通过Θ=Y/X***<br>
***这种方式求出来的，我之前认为是这种方式，才会弄不明白为何拟合出来的线没和数据点完全重合***



